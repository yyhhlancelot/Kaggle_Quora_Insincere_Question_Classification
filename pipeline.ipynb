{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先载入可能需要使用的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "import os\n",
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, MaxPooling1D, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, BatchNormalization, PReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import concatenate, add\n",
    "from keras.callbacks import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理阶段：\n",
    "#### 符号清理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用正则表达式清理数字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 清理错误拼写："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    " \n",
    " \n",
    "mispell_dict = {'colour':'color','centre':'center','didnt':'did not','doesnt':'does not',\n",
    "                'isnt':'is not','shouldnt':'should not','favourite':'favorite','travelling':'traveling',\n",
    "                'counselling':'counseling','theatre':'theater','cancelled':'canceled','labour':'labor',\n",
    "                'organisation':'organization','wwii':'world war 2','citicise':'criticize','instagram': 'social medium',\n",
    "                'whatsapp': 'social medium','snapchat': 'social medium',\"ain't\": \"is not\", \n",
    "                \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \n",
    "                \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "                \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "                \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
    "                \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\"i'll've\": \"i will have\",\n",
    "                \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                \"it's\": \"it is\",\"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "                \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                \"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\", \n",
    "                \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\", \n",
    "                \"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\", \n",
    "                \"so've\": \"so have\",\"so's\": \"so as\",\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "                \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "                \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "                \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \n",
    "                \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \n",
    "                \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "                \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\",\n",
    "                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
    "                \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n",
    "                \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', \n",
    "                'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', \n",
    "                'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', \n",
    "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', \n",
    "                'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', \n",
    "                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', \n",
    "                'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do',\n",
    "                'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \n",
    "                \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', \n",
    "                'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \n",
    "                \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', \n",
    "                'demonitization': 'demonetization', 'demonetisation': 'demonetization'\n",
    "                }\n",
    " \n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    " \n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    " \n",
    "    return mispellings_re.sub(replace, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    " \n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    " \n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 300 #词向量维度\n",
    "max_features = 95000 #设置词典大小\n",
    "max_len = 70 #设置输入的长度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lower\n",
    "train_df['question_text'] = train_df['question_text'].apply(lambda x : x.lower())\n",
    "test_df['question_text'] = test_df['question_text'].apply(lambda x : x.lower())\n",
    " \n",
    "# clean the text\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x : clean_text(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x : clean_text(x))\n",
    " \n",
    "# clean numbers\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x : clean_numbers(x))\n",
    " \n",
    "# clean spellings\n",
    "train_df['question_text'] = train_df['question_text'].apply(lambda x: replace_typical_misspell(x))\n",
    "test_df['question_text'] = test_df['question_text'].apply(lambda x: replace_typical_misspell(x))\n",
    " \n",
    "# fill up the missing values\n",
    "train_X = train_df['question_text'].fillna(\"_##_\").values\n",
    "test_X = test_df['question_text'].fillna(\"_##_\").values\n",
    " \n",
    "# tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    " \n",
    "# pad the sentences\n",
    "train_X = pad_sequences(train_X, maxlen = max_len)\n",
    "test_X = pad_sequences(test_X, maxlen = max_len)\n",
    " \n",
    "# the target values\n",
    "train_y = train_df['target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "trn_idx = np.random.permutation(len(train_X))\n",
    " \n",
    "train_X = train_X[trn_idx]\n",
    "train_y = train_y[trn_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "#     EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    EMBEDDING_FILE = 'J:/Code/kaggle/Quora_Insincere_Question_Classfication/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, 'r', encoding = 'UTF-8'))\n",
    " \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    " \n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    " \n",
    "def load_fasttext(word_index):    \n",
    "#     EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    EMBEDDING_FILE = 'J:/Code/kaggle/Quora_Insincere_Question_Classfication/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, 'r', encoding = 'UTF-8') if len(o)>100)\n",
    " \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    " \n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    " \n",
    "    return embedding_matrix\n",
    " \n",
    "def load_para(word_index):\n",
    "#     EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    EMBEDDING_FILE = 'J:/Code/kaggle/Quora_Insincere_Question_Classfication/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    " \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    " \n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    " \n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    " \n",
    "        self.built = True\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    " \n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    " \n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    " \n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    " \n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    " \n",
    "        eij = K.tanh(eij)\n",
    " \n",
    "        a = K.exp(eij)\n",
    " \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    " \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    " \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胶囊网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    " \n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    " \n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    " \n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    " \n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    " \n",
    "        return outputs\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "    \n",
    "def capsule():\n",
    "    K.clear_session()       \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(rate=0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(100, return_sequences=True, \n",
    "                                kernel_initializer=initializers.glorot_normal(seed=12300), recurrent_initializer=initializers.orthogonal(gain=1.0, seed=10000)))(x)\n",
    " \n",
    "    x = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "    x = Flatten()(x)\n",
    " \n",
    "    x = Dense(100, activation=\"relu\", kernel_initializer=glorot_normal(seed=12300))(x)\n",
    "    x = Dropout(0.12)(x)\n",
    "    x = BatchNormalization()(x)\n",
    " \n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(),)\n",
    "    return model\n",
    " \n",
    "def f1_smart(y_true, y_pred):\n",
    "    args = np.argsort(y_pred)\n",
    "    tp = y_true.sum()\n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过对各路大神的模型进行比较，我找出了几种比较高效的模型\n",
    "\n",
    "#### 首先是使用了注意力机制的双向LSTM/GRU模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_lstm_atten(embedding_matrix):\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(40, return_sequences = True))(x)\n",
    "    y = Bidirectional(CuDNNGRU(40, return_sequences = True))(x)\n",
    "    atten_1 = Attention(max_len)(x)\n",
    "    atten_2 = Attention(max_len)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
    "    conc = Dense(16, activation = \"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(1, activation = \"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = outp)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [f1])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用了注意力机制和胶囊网络的双向LSTM/GRU模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_atten_capsule(embedding_matrix): \n",
    "    '''0.7'''\n",
    "    inp_x = Input(shape = (max_len, ))\n",
    "    inp_features = Input(shape = (6, ))\n",
    "    x = Embedding(max_features,embed_size, weights = [embedding_matrix], trainable = False)(inp_x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    lstm = Bidirectional(CuDNNLSTM(60, return_sequences = True, kernel_initializer = initializers.glorot_normal(seed = 12300),\n",
    "                                   recurrent_initializer=initializers.orthogonal(gain=1.0, seed=10000)))(x)\n",
    "    gru = Bidirectional(CuDNNGRU(60, return_sequences = True, kernel_initializer = initializers.glorot_normal(seed = 12300),\n",
    "                                 recurrent_initializer=initializers.orthogonal(gain=1.0, seed=10000)))(lstm)\n",
    "#     x = Bidirectional(CuDNNLSTM(64, return_sequences = True))(x)\n",
    "    content3 = Capsule(num_capsule = 10, dim_capsule = 10, routings = 4, share_weights = True)(gru)\n",
    "    content3 = Dropout(0.1)(content3)\n",
    "#     content3 = Reshape(-1, )(content3)\n",
    "    content3 = Flatten()(content3)\n",
    "    content3 = Dense(1, activation = \"relu\", kernel_initializer=initializers.glorot_normal(seed=12300))(content3)\n",
    "    ### 修改了content3\n",
    "    atten_lstm = Attention(max_len)(lstm)\n",
    "    atten_gru = Attention(max_len)(gru)\n",
    "    avg_pool = GlobalAveragePooling1D()(gru)\n",
    "    max_pool = GlobalMaxPooling1D()(gru)\n",
    "    \n",
    "   \n",
    "    conc = concatenate([atten_lstm, atten_gru, content3, avg_pool, max_pool, inp_features]) #\n",
    "    \n",
    "    ### 修改了dense\n",
    "    conc = Dense(16, activation = \"relu\", kernel_initializer=initializers.glorot_normal(seed=12300))(conc)\n",
    "    x = BatchNormalization()(conc)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outp = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs = [inp_x, inp_features], outputs = outp)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [f1])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn(embedding_matrix):\n",
    "    filter_sizes = [1, 2, 3, 5]\n",
    "    num_filters = 36\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix])(inp)\n",
    "    x = Reshape((max_len, embed_size, 1))(x)\n",
    "    \n",
    "    maxpool_pool = []\n",
    "    \n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size = (filter_sizes[i], embed_size), kernel_initializer = 'he_normal', activation = 'elu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size = (max_len - filter_sizes[i] + 1, 1))(conv))\n",
    "    \n",
    "    z = Concatenate(axis = 1)(maxpool_pool)\n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "    outp = Dense(1, activation = \"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = outp)\n",
    "    model.summary()\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以及我在网上找到了腾讯之前提出的dpCNN模型并进行了一些针对性的修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_dpcnn(embedding_matrix):\n",
    "    filter_nr = 64 \n",
    "    filter_size = 3\n",
    "    max_pool_size = 3\n",
    "    max_pool_strides = 2\n",
    "    dense_nr = 256\n",
    "    spatial_dropout = 0.1\n",
    "    dense_dropout = 0.2\n",
    "    train_embed = False\n",
    "    conv_kern_reg = regularizers.l2(0.00001)\n",
    "    conv_bias_reg = regularizers.l2(0.00001)\n",
    "    \n",
    "    inp = Input(shape = (max_len, ))\n",
    "    emb_comment = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "#     emb_comment = SpatialDropout1D(0.1)(emb_comment)\n",
    "    \n",
    "    #block1\n",
    "    block1 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(emb_comment)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = PReLU()(block1)\n",
    "    block1 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = PReLU()(block1)\n",
    "    #we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n",
    "    #if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n",
    "    resize_emb = Conv1D(filter_nr, kernel_size = 1, padding = 'same', activation = 'linear', \n",
    "                       kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(emb_comment)\n",
    "    resize_emb = PReLU()(resize_emb)\n",
    "    \n",
    "    block1_output = add([block1, resize_emb])\n",
    "    block1_output = MaxPooling1D(pool_size = max_pool_size, strides = max_pool_strides)(block1_output)\n",
    "    \n",
    "    \n",
    "    #block2\n",
    "    block2 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block1_output)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = PReLU()(block2)\n",
    "    block2 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block2)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = PReLU()(block2)\n",
    "    \n",
    "    block2_output = add([block2, block1_output])\n",
    "    block2_output = MaxPooling1D(pool_size = max_pool_size, strides = max_pool_strides)(block2_output)\n",
    "    \n",
    "    #block3\n",
    "    block3 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block2_output)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = PReLU()(block3)\n",
    "    block3 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block3)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = PReLU()(block3)\n",
    "    \n",
    "    block3_output = add([block3, block2_output])\n",
    "    block3_output = MaxPooling1D(pool_size = max_pool_size, strides = max_pool_strides)(block3_output)\n",
    "    \n",
    "    #block4\n",
    "    block4 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block3_output)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = PReLU()(block4)\n",
    "    block4 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block4)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = PReLU()(block4)\n",
    "    \n",
    "    block4_output = add([block4, block3_output])\n",
    "    block4_output = MaxPooling1D(pool_size = max_pool_size, strides = max_pool_strides)(block4_output)\n",
    "    \n",
    "    #block5\n",
    "    block5 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block4_output)\n",
    "    block5 = BatchNormalization()(block5)\n",
    "    block5 = PReLU()(block5)\n",
    "    block5 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block5)\n",
    "    block5 = BatchNormalization()(block5)\n",
    "    block5 = PReLU()(block5)\n",
    "    \n",
    "    block5_output = add([block5, block4_output])\n",
    "    block5_output = MaxPooling1D(pool_size = max_pool_size, strides = max_pool_strides)(block5_output)\n",
    "    \n",
    "#     #block6\n",
    "#     block6 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "#                    kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block5_output)\n",
    "#     block6 = BatchNormalization()(block6)\n",
    "#     block6 = PReLU()(block6)\n",
    "#     block6 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "#                    kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block6)\n",
    "#     block6 = BatchNormalization()(block6)\n",
    "#     block6 = PReLU()(block6)\n",
    "    \n",
    "#     block6_output = add([block6, block5_output])\n",
    "#     block6_output = MaxPooling1D(pool_size = max_pool_size, strides = max_pool_strides)(block6_output)\n",
    "    \n",
    "    #block7\n",
    "    block7 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block5_output)\n",
    "    block7 = BatchNormalization()(block7)\n",
    "    block7 = PReLU()(block7)\n",
    "    block7 = Conv1D(filter_nr, kernel_size = filter_size, padding = 'same', activation = 'linear',\n",
    "                   kernel_regularizer = conv_kern_reg, bias_regularizer = conv_bias_reg)(block7)\n",
    "    block7 = BatchNormalization()(block7)\n",
    "    block7 = PReLU()(block7)\n",
    "    \n",
    "    block7_output = add([block7, block5_output])\n",
    "    outp = GlobalMaxPooling1D()(block7_output)\n",
    "#     output = block7_output\n",
    "    \n",
    "    outp = Dense(dense_nr, activation = 'linear')(outp)\n",
    "    outp = BatchNormalization()(outp)\n",
    "    outp = Dropout(0.1)(outp)\n",
    "    outp = Dense(1, activation = 'sigmoid')(outp)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = outp)\n",
    "    model.summary()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                 optimizer = 'adam',\n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时还有一些模型，大致的思路也是用了注意力机制和胶囊网络以及双向LSTM或者GRU，只是模型的构造不同，这里就没有举出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量处理与训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_1 = load_glove(tokenizer.word_index)\n",
    "# embedding_matrix_2 = load_fasttext(tokenizer.word_index)\n",
    "embedding_matrix_3 = load_para(tokenizer.word_index)\n",
    " \n",
    "embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_3], axis = 0)\n",
    "# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2], axis = 0)\n",
    "# np.shape(embedding_matrix)\n",
    "del embedding_matrix_1, embedding_matrix_3\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查找最佳阈值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_prob):\n",
    "    best_thresh = 0\n",
    "    best_score = 0\n",
    "    for thresh in np.arange(0.1, 0.701, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        score = metrics.f1_score(y_true, (y_prob >= thresh).astype(int))\n",
    "        print(\"F1 score at threshold {} is {}\".format(thresh, metrics.f1_score(y_true, (y_prob >= thresh).astype(int))))\n",
    "        if score > best_score : \n",
    "            best_score = score\n",
    "            best_thresh = thresh\n",
    "    return best_thresh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_pred(model, dev_X, dev_y, val_X, val_y, test_X, dev_features = None, val_features = None, epochs = None, callback = None):\n",
    "    if dev_features is None:\n",
    "        model.fit(dev_X, dev_y, batch_size = 512, epochs = epochs, validation_data = (val_X, val_y), callbacks = callback, verbose = 0)\n",
    "        pred_test_y_temp = model.predict(test_X, batch_size = 1024)\n",
    "#     pred_test_y_temp = model.predict(np.concatenate((test_X, test_features), axis = 1), batch_size = 1024)\n",
    "    else:\n",
    "        model.fit([dev_X, dev_features], dev_y, batch_size = 512, epochs = epochs, validation_data = ([val_X, val_features], val_y), callbacks = callback, verbose = 0)\n",
    "        pred_test_y_temp = model.predict([test_X, test_features], batch_size = 1024)\n",
    "    return pred_test_y_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我用了四折交叉验证，并使用了其中一个模型来训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ADDITION TRAIN lstm_atten\n",
    "num_splits = 4\n",
    "skf = StratifiedKFold(n_splits = num_splits, shuffle = True, random_state = 2333)\n",
    "pred_test_y = 0\n",
    "thresh_use = 0\n",
    "val_score = 0\n",
    "for dev_index, val_index in skf.split(train_X, train_y):\n",
    "    dev_X, val_X = train_X[dev_index, :], train_X[val_index,:]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "#     dev_features, val_features = train_features[dev_index, :], train_features[val_index, :]\n",
    "    model = model_lstm_atten(embedding_matrix)\n",
    "    pred_test_y_temp = train_pred(model, dev_X, dev_y, val_X, val_y, test_X, dev_features = None, val_features = None, epochs = 2, callback = [clr,])\n",
    "    pred_val_y = model.predict(val_X, batch_size = 1024)\n",
    "    best_thresh = threshold_search(val_y, pred_val_y)\n",
    "    val_score_temp = metrics.f1_score(val_y, (pred_val_y > best_thresh).astype(int))\n",
    "    print(\"val temp best f1 score is {0} and best thresh is {1}\".format(val_score_temp, best_thresh))\n",
    "    \n",
    "    thresh_use += best_thresh\n",
    "    pred_test_y += pred_test_y_temp\n",
    "    val_score += val_score_temp\n",
    "    keras.backend.clear_session()\n",
    "pred_test_y /= num_splits\n",
    "thresh_use /= num_splits\n",
    "val_score /= num_splits\n",
    "output.append([pred_test_y, thresh_use, val_score, 'lstm atten glove+para'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub.prediction = (pred_test_y > thresh_use).astype(int)\n",
    "sub.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于这个比赛的结果是通过Kernel进行线上提交，也就是说运行时间不能超过两个小时，所以要合理利用时间，在此基础上可能需要对策略进行修改，比如交叉验证的折数，epoch的数量的等。以上基本上就是该次比赛我的全部流程。希望下次能比这次更好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
